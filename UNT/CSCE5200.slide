Beyond Bag of Words
Improving Text Classification by Using Encyclopedia Knowledge
13 Apr 2017

Jacob Hochstetler
CSCE5200 Information Retrieval
jacobhochstetler@my.unt.edu
http://github.com/jh125486

* Organization

1. Introduction

2. Related Works

3. Wikipedia

4. Text Document Representation

5. Experiments

6. Conclusion/Future Works

* Introduction

* Traditional document classification

*Bag*of*Words* (_BOW_) = weighted term frequency

- Good: Fast (single document scan)

- Bad: Uses only direct text (no synonyms)

- Ugly: Only works within the document composition (relationships)
.image images/UNT_green.png 3 850

Next Step: Improve _BOW_ by exploiting term *ontology*...

*Ontology* (Information Science context) here meaning:
"formal naming..., properties, and interrelationships of the entities" - [Wikipedia]

* Related Works: background

_WordNet_ is a manually created structured term ontology

- Created at Cognitive Science Laboratory (Princeton) in 1985
- Groups English words into sets of synonyms (synsets)
- Combination of dictionary and thesaurus
- Primary use is in automatic text analysis and artificial intelligence applications

* Related Works: B. Rodriguez et al. and U. Loez et al. 
Successful integration of _WordNet_:

- Supervised scenario; built term vectors manually
- Did not user hypernyms or associate terms
- Improved classification

* Related Works: Dave et al. 
Utilized _WordNet_ synsets as features:

- No word sense disambiguation
- Experimentally decreased clustering performance

* Related Works: Gabrilovich et al. 
Built a text classification system with encyclopedic knowledge:

- Classifier that matches documents with the most relevant articles of _Wikipedia_
- Did not use full set of _Wikipedia_ features
- Improved classification

* Wikipedia: Introduction 
The *Largest* Encyclopedia in the _World_

- Launched in 2001
- Multilingual, web-based, free content encyclopedia
- Organizes content (hierarchy categories) much like traditional thesauri
.image images/UNT_green.png 2 850

By November 30, 2006:

- 4,197,766 articles 
- Each article is a single topic (with a succinct title)
- Full content and revision history: 70 GB (compressed)
 
* Wikipedia: Article features 
As the *Largest* Thesaurus in the _World_...

- Synonymy (same meaning, different term) using _Redirect_links_ 
- Polysemy (same word, different meaning) using _Disambiguation_
- Hyponymy (hierarchical relation) using _Categories_
- Associative relations (relatedness) using _hyperlinks_
.image images/UNT_green.png 3 850

For our use:

- Identified >4 million distinct "entities"
- Organized into 120,000 categories
- Each with an average of 2 subcategories and 26 articles

* Associative relations
Three kinds of measurements to rank article links:

1. *Content*based* using _TFIDF_ between two documents

2. *Out-link*Category*based* using shared category out-links

3. *Distance*based* using the edge distance between two nodes

.image images/UNT_green.png 2 850

*Problem*: 
How to combine these measures into an overall similarity index?

*Solution*: 
.image images/s_index.png
.caption _Where_Î»_are_weight_parameters_experimentally_tuned_



* Text Document Representation

* Experiments

* Conclusion/Future Works

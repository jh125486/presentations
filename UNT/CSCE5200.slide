Beyond Bag of Words
Improving Text Classification by Using Encyclopedia Knowledge
13 Apr 2017

Jacob Hochstetler
CSCE5200 Information Retrieval
jacobhochstetler@my.unt.edu
http://github.com/jh125486

* Organization

1. Introduction

2. Related Works

3. Wikipedia

4. Compiling Wikipedia Knowledge into Text Document Representation

5. Experiments

6. Conclusion/Future Works

* Introduction

* Traditional document classification

*Bag*of*Words* (_BOW_) = weighted term frequency

- *Good*: Fast (single document scan)

- *Bad*: Uses only direct text (no synonyms)

- *Ugly*: Only works within the document composition (no relationships)
.image images/UNT_green.png 3 850

*Next*Step*: Improve _BOW_ by exploiting term *ontology*...

*Ontology* (Information Science context) here meaning:
_"formal_naming...,_properties,_and_interrelationships_of_the_entities"_- [Wikipedia]

* Related Works
* Background

_WordNet_ is a manually created structured term ontology

- Created at Cognitive Science Laboratory (Princeton) in 1985
- Groups English words into sets of synonyms (synsets)
- Combination of dictionary and thesaurus
- Primary use is in automatic text analysis and artificial intelligence applications

* B. Rodriguez et al. and U. Loez et al. 
Successful integration of _WordNet_:

- Supervised scenario; built term vectors manually
- Did not user hypernyms or associate terms
- Improved classification

* Dave et al. 
Utilized _WordNet_ synsets as features:

- No word sense disambiguation
- Experimentally decreased clustering performance

* Gabrilovich et al. 
Built a text classification system with encyclopedic knowledge:

- Classifier that matches documents with the most relevant articles of _Wikipedia_
- Did not use full set of _Wikipedia_ features
- Improved classification

* Wikipedia

* Introduction 
The *Largest* Encyclopedia in the _World_

- Launched in 2001
- Multilingual, web-based, free content encyclopedia
- Organizes content (hierarchy categories) much like traditional thesauri
.image images/UNT_green.png 2 850

By November 30, 2006:

- 4,197,766 articles 
- Each article is a single topic (with a succinct title)
- Full content and revision history: 70 GB (compressed)
 
* Article features 
As the *Largest* Thesaurus in the _World_...

- Synonymy (same meaning, different term) using _Redirect_links_ 
- Polysemy (same word, different meaning) using _Disambiguation_
- Hyponymy (hierarchical relation) using _Categories_
- Associative relations (relatedness) using _hyperlinks_
.image images/UNT_green.png 3 850

For our use:

- Identified >4 million distinct "entities"
- Organized into 120,000 categories
- Each with an average of 2 subcategories and 26 articles

* Associative relations
Three kinds of measurements to rank article links:

1. *Content*based* using _TFIDF_ between two documents

2. *Out-link*Category*based* using shared category out-links

3. *Distance*based* using the edge distance between two nodes

.image images/UNT_green.png 2 850

*Problem*: 
How to combine these measures into an overall similarity index?

*Solution*: 
.image images/s_index.png
.caption _Where_位_are_experimentally_tuned_weight_parameters_

* Compiling Wikipedia Knowledge into Text Document Representation

* Text Document Representation: A weighted bag of terms

Iterate each document:
1. Remove Stopwords
2. Stem using Porter
3. Construct Vector of terms
4. Weight the frequency of each term using _TFIDF_:
.image images/tfidf.png
.caption Where _DF_ (_t_) is the document frequency of term _t_ that counts in how many documents where term _t_ appears

* Text Document Enrichment
.image images/text_enrichment_procedure.png 570 _

* Procedure
Indexing _Wikipedia_ concepts:

- Remove useless titles
- Remove chronology categories
- Heuristic eval whether *Title* is a _concept_

Search _Wikipedia_ concepts in documents:

1. Split the document into vectors of term sequence
2. Find candidate concepts in each term sequence via window filtering condition.
3. Filter candidate concepts removing ones subsumed by other candidate concepts.

Add _Wikipedia_ concepts into document:

- Disambiguation with text similarity from cosine similarity of the TFIDF vectors
- Disambiguation with context using conceptual distance function


* Experiments

- Used Reuters-21578, OHSUMED (subset of MEDLINE), 20 Newsgroups
- Tuned 位 weights (位1 set to 0.4 and 位2 set to 0.5)

* Results

* Hyponymies
.image images/hyponymies.png

* Associative Concepts
.image images/associative_concepts.png

* Synonymies
.image images/synonymies.png

* Final: Hyponymies and Associative Concepts
.image images/hyponymies_and_associative_concepts.png


* Conclusion/Future Works

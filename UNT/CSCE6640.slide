Modern Distributed Computing
A Grab Bag of Stuff
1 May 2018

Jacob Hochstetler
CSCE6640
jacobhochstetler@my.unt.edu
http://github.com/jh125486

* Organization

1. Introduction & Background

2. The Network is the Computer, also the Problem

3. Raft

4. MapReduce

* Introduction

The world's getting pretty big.

* Communication Capacity

The world's effective information capacity through telecommunication networks was:

- 1986: 281 petabytes
- 1993: 471 petabytes 
- 2000: 2,200 petabytes
- 2007: 65,000 petabytes

* Compute/Memory Capacity

.image images/computer-brain.jpeg _ 750

* Background - Rise of the Cloud™

- On-demand self-service (across heterogenous access models)
- Resource pooled (multi-tenant & metered/quotaed)
- Rapid elasticity
.caption [[https://dx.doi.org/10.6028/NIST.SP.800-145]["The NIST Definition of Cloud Computing", National Institute of Standards and Technology]]

* Background - Rise of the Cloud™

1. *IaaS* - _Infrastructure_ as a Service

2. *PaaS* - _Platform_ as a Service

3. *SaaS* - _Software_ as a Service

* Background - Rise of the Cloud™... as a Service

.image images/pizza_as_a_service.png _ 1040

* Background - Rise of the Cloud™

Other computing models:

- Edge
- Fog
- Mist/IoT

* The Network is the Computer, also the Problem

* The Fallacies of Distributed Computing

1. The network is reliable.

: Software applications are written with little error-handling on networking errors. During a network outage, such applications may stall or infinitely wait for an answer packet, permanently consuming memory or other resources. When the failed network becomes available, those applications may also fail to retry any stalled operations or require a (manual) restart.

2. Latency is zero.

: Ignorance of network latency, and of the packet loss it can cause, induces application- and transport-layer developers to allow unbounded traffic, greatly increasing dropped packets and wasting bandwidth.

3. Bandwidth is infinite.

: Ignorance of bandwidth limits on the part of traffic senders can result in bottlenecks over frequency-multiplexed media.

4. The network is secure.

: Complacency regarding network security results in being blindsided by malicious users and programs that continually adapt to security measures.[2]

5. Topology doesn't change.

: Changes in network topology can have effects on both bandwidth and latency issues, and therefore similar problems.

6. There is one administrator.

: Multiple administrators, as with subnets for rival companies, may institute conflicting policies of which senders of network traffic must be aware in order to complete their desired paths.

7. Transport cost is zero.

: The "hidden" costs of building and maintaining a network or subnet are non-negligible and must consequently be noted in budgets to avoid vast shortfalls.

8. The network is homogeneous.

: If a system assumes a homogeneous network, then it can lead to the same problems that result from the first three fallacies.

.caption ~1994 Sun Microsystems fellows

* Database Replication

1. Scalability

2. Latency

3. Recovery (Disaster Tolerance/Continuity of Operations)

Multiple copies == multiple problems...

* Consistency Models

Guarantee by the datastore that certain invariants will hold for reads and/or writes.

Standard consistency models (weakest to strongest):

- Eventual Consistency
: All will eventual converge. No guarantees on order of writes applied to different replicas. No guarantees on what intermediate states a reader may observe.
- Session Consistency Guarantees
: Persistence across a context. A popular session guarantee is Read-your-writes, which means that within a session you can read any data that you have written.
- Causal Consistency
: Guarantees that operations will be applied to the datastore in causal order. Post -> Comment -> Comment.
- Serializability
: Serializability means that operations appear as if there was a global total order or equivalently that there was a single copy of the data. This has the implication that a read of an object returns the latest write to that object, even if the write was on a different replica.

* CAP Theorem

Formalizes the trilemma that happens after a network partition occurs (split-brain).

Originally published by Eric Brewer in 1999 and formally proven at MIT ~2002.
.caption [[https://dl.acm.org/citation.cfm?id=564601]["Brewer's conjecture and the feasibility of consistent, available, partition-tolerant web services", ACM SIGACT Vol. 33-2]]

.image UNT_green.png 1 850

- *Consistency*: A read is guaranteed to return the most recent write for a given client.

- *Availability*: A non-failing node will return a reasonable response within a reasonable amount of time (no error or timeout).

- *Partition*Tolerance*: The system will continue to function when network partitions occur.

* CAP Theorem Trilemma

.image images/CAP-Theorem.png

* PACELC Theorem

Formalized as an extension to CAP by Daniel Abadi at Yale in 2012.
.caption [[https://dl.acm.org/citation.cfm?id=2360959]["Consistency Tradeoffs in Modern Distributed Database System Design", Computer Vol. 45-2]]
.caption Also of note: [[https://dl.acm.org/citation.cfm?id=1920855]["The case for determinism in database systems", VLDB Endowment Vol. 3-1/2]]

.image UNT_green.png 1 850

Whereas (E)ven when under normal operating conditions (no network partitions), there has to be a choice between latency (L) and consistency (C).

.image images/pacelc.png

* Raft 

* Consensus Protocols

The process of agreeing on one result among a group of participating nodes.

Difficulty increases when this process moves to networks, which are inherently unreliable.

Forms the basis for state machine replication, a technique for converting an algorithm into a fault-tolerant, distributed implementation.

* Paxos

First published in 1989 by Leslie Lamport, and the standard for over a decade.

  "Famously difficult to reason about"
  "Non-intuitive"
  "Underspecification"

_Paxos_ is ridiculously complicated. 

.image images/paxos.jpg _ 900
.caption Artist's Interpretation of Paxos consensus protocol.

* Raft 

An alternative to Lamport's _Paxos_ protocol for distributed consensus.

Two major benefits over _Paxos_:
1. Human (non-Leslie Lamport) understandable
2. Thereby human implementable ([[https://raft.github.io/][~100 implementations in 24 languages in 4 years]])

.image images/raft.png _ 1000
.caption [[https://dl.acm.org/citation.cfm?id=2643666]["In Search of an Understandable Consensus Algorithm", USENIX '14]]
.caption Also of note: [[https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-857.pdf]["ARC: Analysis of Raft Consensus", University of Cambridge, Technical Report 857]]

* Raft Distributed Consensus Example

.iframe https://thesecretlivesofdata.com/raft/ 600 1000

* MapReduce

* MapReduce

Programming model for dealing with large big data sets, consisting of two parts (_mostly_):

: 1. an input reader
: 2. a Map function
: 3. a partition function
: 4. a compare function
: 5. a Reduce function
: 6. an output writer

1. Map: _f_(x) => *(key,* *x)*

2. Reduce: filter/aggregate{x₁,x₂...xₙ} => *x′*

.image images/mapreduce-workflow.png _ 650

.caption [[https://dl.acm.org/citation.cfm?id=1251264]["MapReduce: Simplified Data Processing on Large Clusters", OSDI'04 Vol. 6]]

* MapReduce Archetypal Example

The _"Hello_World"_ of Big Data... Counting input word frequency.

.image images/MapReduce_Work_Structure2.png _ 1000

The _"Hello_World"_ of Big Data... Counting input word frequency.

* Data Locality the Data-centric computing ecosystem

- Compute is cheap, massively parallel and elastic.
- The Network is finite, both in bandwidth/throughput and latency/jitter.
- Therefore the True Cost™ of computing is moving the data.

.image images/hadoop-ecosystem.png _ 950

* Hadoop's HDFS/AWS EMRFS

Rack-aware data availability mapper preference:

1. Data local 
2. Intra-rack
3. Inter-rack

.image images/hdfs.jpg _ 800

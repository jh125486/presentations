Modern Distributed Computing
A Grab Bag of Stuff
1 May 2018

Jacob Hochstetler
CSCE6640
jacobhochstetler@my.unt.edu
http://github.com/jh125486

* Organization

1. Introduction & Background

2. The Network is the Computer, also the Problem

3. Consensus Protocols

4. MapReduce

* Introduction

The world's getting pretty big.

* Communication Capacity

The world's effective information capacity through telecommunication networks was:

- 1986: 281 petabytes
- 1993: 471 petabytes
- 2000: 2,200 petabytes
- 2007: 65,000 petabytes

* Compute/Memory Capacity

.image images/computer-brain.jpeg _ 750

* Background - Rise of the Cloud™

- On-demand self-service (across heterogenous access models)
- Resource pooled (multi-tenant & metered/quotaed)
- Rapid elasticity
.caption [[https://dx.doi.org/10.6028/NIST.SP.800-145]["The NIST Definition of Cloud Computing", National Institute of Standards and Technology]]

* Background - Rise of the Cloud™

1. *IaaS* - _Infrastructure_ as a Service

2. *PaaS* - _Platform_ as a Service

3. *SaaS* - _Software_ as a Service

* Background - Rise of the Cloud™... as a Service

.image images/pizza_as_a_service.png _ 1040

* Background - Rise of the Cloud™

Other computing models:

- Edge
- Fog
- Mist/IoT

* The Network is the Computer, also the Problem

* The Fallacies of Distributed Computing

1. The network is reliable.

: Software applications are written with little error-handling on networking errors. During a network outage, such applications may stall or infinitely wait for an answer packet, permanently consuming memory or other resources. When the failed network becomes available, those applications may also fail to retry any stalled operations or require a (manual) restart.

2. Latency is zero.

: Ignorance of network latency, and of the packet loss it can cause, induces application- and transport-layer developers to allow unbounded traffic, greatly increasing dropped packets and wasting bandwidth.

3. Bandwidth is infinite.

: Ignorance of bandwidth limits on the part of traffic senders can result in bottlenecks over frequency-multiplexed media.

4. The network is secure.

: Complacency regarding network security results in being blindsided by malicious users and programs that continually adapt to security measures.[2]

5. Topology doesn't change.

: Changes in network topology can have effects on both bandwidth and latency issues, and therefore similar problems.

6. There is one administrator.

: Multiple administrators, as with subnets for rival companies, may institute conflicting policies of which senders of network traffic must be aware in order to complete their desired paths.

7. Transport cost is zero.

: The "hidden" costs of building and maintaining a network or subnet are non-negligible and must consequently be noted in budgets to avoid vast shortfalls.

8. The network is homogeneous.

: If a system assumes a homogeneous network, then it can lead to the same problems that result from the first three fallacies.

.caption ~1994 Sun Microsystems fellows

* Database Replication

1. Scalability

2. Latency

3. Recovery (Disaster Tolerance/Continuity of Operations)

More copies, More problems...

* Consistency Models

Guarantee by the datastore that certain invariants will hold for reads and/or writes.

Standard consistency models (weakest to strongest):

- Eventual Consistency
: All will eventual converge. No guarantees on order of writes applied to different replicas. No guarantees on what intermediate states a reader may observe.
- Session Consistency Guarantees
: Persistence across a context. A popular session guarantee is Read-your-writes, which means that within a session you can read any data that you have written.
- Causal Consistency
: Guarantees that operations will be applied to the datastore in causal order. Post -> Comment -> Comment.
- Serializability
: Serializability means that operations appear as if there was a global total order or equivalently that there was a single copy of the data. This has the implication that a read of an object returns the latest write to that object, even if the write was on a different replica.

* CAP Theorem

Formalizes the trilemma that happens after a network partition occurs (split-brain).

Originally published by Eric Brewer in 1999 and formally proven at MIT ~2002.
.caption [[https://dl.acm.org/citation.cfm?id=564601]["Brewer's conjecture and the feasibility of consistent, available, partition-tolerant web services", ACM SIGACT Vol. 33-2]]

.image UNT_green.png 1 850

- *Consistency*: A read is guaranteed to return the most recent write for a given client.

- *Availability*: A non-failing node will return a reasonable response within a reasonable amount of time (no error or timeout).

- *Partition*Tolerance*: The system will continue to function when network partitions occur.

* CAP Theorem Trilemma

.image images/CAP-Theorem.png

* PACELC Theorem

Formalized as an extension to CAP by Daniel Abadi at Yale in 2012.
.caption [[https://dl.acm.org/citation.cfm?id=2360959]["Consistency Tradeoffs in Modern Distributed Database System Design", Computer Vol. 45-2]]
.caption Also of note: [[https://dl.acm.org/citation.cfm?id=1920855]["The case for determinism in database systems", VLDB Endowment Vol. 3-1/2]]

.image UNT_green.png 1 850

Whereas (E)ven when under normal operating conditions (no network partitions), there has to be a choice between latency (L) and consistency (C).

.image images/pacelc.png

* Consensus Protocols

* Consensus Protocols

The process of agreeing on one result among a group of participating nodes.

Difficulty increases when this process moves to networks, which are inherently unreliable.

Forms the basis for state machine replication, a technique for converting an algorithm into a fault-tolerant, distributed implementation.

* Paxos

First published in 1989 by Leslie Lamport, and the standard for over a decade.

  "Famously difficult to reason about"
  "Non-intuitive"
  "Underspecification"

_Paxos_ is ridiculously complicated.

.image images/paxos.jpg _ 900
.caption Artist's Interpretation of Paxos consensus protocol.

* Raft

An alternative to Lamport's _Paxos_ protocol for distributed consensus.

Two major benefits over _Paxos_:
1. Human (non-Leslie Lamport) understandable
2. Thereby human implementable ([[https://raft.github.io/][~100 implementations in 24 languages in 4 years]])

.image images/raft.png _ 1000
.caption [[https://dl.acm.org/citation.cfm?id=2643666]["In Search of an Understandable Consensus Algorithm", USENIX '14]]
.caption Also of note: [[https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-857.pdf]["ARC: Analysis of Raft Consensus", University of Cambridge, Technical Report 857]]

* Raft Distributed Consensus Example

.iframe https://jh125486.github.io/raft/ 600 1000

* MapReduce

* MapReduce

Programming model for dealing with large big data sets, consisting of two parts (_mostly_):

: 1. an input reader
: 2. a Map function
: 3. a partition function
: 4. a compare function
: 5. a Reduce function
: 6. an output writer

1. Map: _f_(x) => *(key,* *x)*

2. Reduce: filter/aggregate{x₁,x₂...xₙ} => *x′*

.image images/mapreduce-workflow.png _ 650

.caption [[https://dl.acm.org/citation.cfm?id=1251264]["MapReduce: Simplified Data Processing on Large Clusters", OSDI'04 Vol. 6]]

* MapReduce Archetypal Example Overview

The _"Hello_World"_ of Big Data... Counting input word frequency.

.image images/MapReduce_Work_Structure2.png _ 1000

The _"Hello_World"_ of Big Data... Counting input word frequency.

* Hadoop Streaming Python Code Example (Mapper)

[[http://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html][_Hadoop_Streaming_API_]] enables any pipeline that can read/write to `STDIN/STDOUT`.

  #!/usr/bin/env python
  """ mapper.py """

  import sys

  # input comes from STDIN (standard input)
  for line in sys.stdin:
      # remove leading and trailing whitespace
      line = line.strip()

      # split the line into words
      words = line.split()

      # increase counters
      for word in words:
          # write the results to STDOUT (standard output);
          # what we output here will be the input for the Reduce step, i.e. the input for reducer.py

          # tab-delimited; the trivial word count is 1
          print '%s\t%s' % (word, 1)

* Hadoop Streaming Python Code Example (Reducer)

  #!/usr/bin/env python
  """ reducer.py """
  from operator import itemgetter
  import sys

  current_word = None
  current_count = 0
  word = None

  for line in sys.stdin:
      word, count = line.strip().split('\t', 1) # parse the input we got from mapper.py
      count = int(count) # convert count (currently a string) to int

  # Only works because Hadoop sorts map output by key (here: word) before it is sent to the reducer
      if current_word == word:
          current_count += count
      else:
          if current_word:
              print '%s\t%s' % (current_word, current_count)
          current_count = count
          current_word = word

  if current_word == word:  # do not forget to output the last word if needed!
      print '%s\t%s' % (current_word, current_count)

* Hadoop Streaming Python Code Example (Testing Locally)

Testing a MapReduce job locally on small data is a huge benefit of the Streaming API.

Finding a mistake 10 minutes into a local pipeline is a magnitude better than 8 hours deep into a Hadoop job.

  $ echo "foo foo quux labs foo bar quux" | ./mapper.py
  foo     1
  foo     1
  quux    1
  labs    1
  foo     1
  bar     1
  quux    1

  $ echo "foo foo quux labs foo bar quux" | ./mapper.py | sort -k1,1 | ./reducer.py
  bar     1
  foo     3
  labs    1
  quux    2

* Hadoop Streaming Python Code Example (on Hadoop)
Copy data to HDFS

  $ bin/hadoop dfs -copyFromLocal /tmp/lots_of_books /user/hadoop/lots_of_books

Run the MapReduce job

  $ bin/hadoop jar contrib/streaming/hadoop-*streaming*.jar \
    -file ./mapper.py    -mapper ./mapper.py \
    -file ./reducer.py   -reducer ./reducer.py \
    -input /user/hadoop/lots_of_books/* \
    -output /user/hadoop/lots_of_books-output

* Hadoop Streaming Python Code Example (Results)

 additionalConfSpec_:null
 null=@@@userJobConfProps_.get(stream.shipped.hadoopstreaming
 packageJobJar: [/app/hadoop/tmp/hadoop-unjar54543/]
 [] /tmp/streamjob54544.jar tmpDir=null
 [...] INFO mapred.FileInputFormat: Total input paths to process : 7
 [...] INFO streaming.StreamJob: getLocalDirs(): [/app/hadoop/tmp/mapred/local]
 [...] INFO streaming.StreamJob: Running job: job_200803031615_0021
 [...] INFO streaming.StreamJob:  map 0%  reduce 0%
 [...] INFO streaming.StreamJob:  map 43%  reduce 0%
                      ...
 [...] INFO streaming.StreamJob:  map 100%  reduce 70%
 [...] INFO streaming.StreamJob:  map 100%  reduce 77%
 [...] INFO streaming.StreamJob:  map 100%  reduce 100%
 [...] INFO streaming.StreamJob: Job complete: job_200803031615_0021
 [...] INFO streaming.StreamJob: Output: /user/hadoop/lots_of_books-output

Result output

  $ bin/hadoop dfs -cat /user/hadoop/lots_of_books-output/part-00000
  "(Lo)cra"       1
  "1490   1
  "1498," 1
  "40,"   1

* Hadoop Streaming Python Code Example (Web UI)

Web UI available at:

- [[http://http://hostname:50030]] => JobTracker
- [[http://http://hostname:50060]] => TaskTracker
- [[http://http://hostname:50070]] => NameNode

* Hadoop Streaming Python Code Example (Web UI example)

.image images/Hadoop-web-interface-screenshot.png _ 500

* Data Locality the Data-centric computing ecosystem

- Compute is cheap, massively parallel and elastic.
- The Network is finite, both in bandwidth/throughput and latency/jitter.
- Therefore the True Cost™ of computing is moving the data.

.image images/hadoop-ecosystem.png _ 950

* Hadoop's HDFS/AWS EMRFS

Rack-aware data availability mapper preference:

1. Data local
2. Intra-rack
3. Inter-rack

.image images/hdfs.jpg _ 800

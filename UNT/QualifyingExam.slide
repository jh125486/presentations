Ph.D. Qualifying Exam
3 May 2017

Jacob Hochstetler
Ph.D. student, University of North Texas

*Committee*members*: Dr. Fu (_advisor_), Dr. Garlick, Dr. Mikler, Dr. Nielsen

jacobhochstetler@my.unt.edu
http://github.com/jh125486

* Introduction

_A_Middleware_Infrastructure_for_Utility-based_Provisioning_of_IoT_Cloud_Systems_
Stefan Nastic, Hong-Linh Truong, and Schahram Dustdar
*2016*IEEE/ACM*Symposium*on*Edge*Computing*(SEC)*
`pages`28-40`

_ECC:_Edge_Cloud_Composites_
Ketan Bhardwaj, Sreenidhy Sreepathy, Ada Gavrilovska and Karsten Schwan
*2014*2nd*IEEE*International*Conference*on*Mobile*Cloud*Computing,*Services,*and*Engineering*
`pages`38-47`

_ImageNet_Classification_with_Deep_Convolutional_Neural_Networks_
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton
*2012*Proceedings*of*the*25th*International*Conference*on*Neural*Information*Processing*Systems*(NIPS)*
`pages`1097-1105`

* 1. A Middleware Infrastructure for Utility-based Provisioning of IoT Cloud Systems

* Organization

1. Introduction

2. Motivation & Background

3. Internet of Things (IoT) Cloud Provisioning Middleware

4. Runtime Mechanisms for Multi-level Provisioning in IoT Cloud

5. Implementation & Evaluation

6. Conclusion

* Introduction

* Differences between Cloud, Fog, and Edge computing

In _Cloud_ computing, sensor *data* is sent *directly* to the Cloud and *processing* happens there.

In _Fog_/_Edge_ computing, *processing* of data is done *onsite* without Cloud involvement.
Although aggregate data may be *forwarded* on to the Cloud for further processing.

*Differences* between _Fog_ and _Edge_:

- _Fog_: sensor data is processed in a separate, locally connected (LAN) node.
- _Edge_: data processing happens in each individual sensor/device.

* IoT Cloud Systems

The *Cloud* defined (per _IETF_):

- Abstracted
- Self-service (_on-demand_)
- Utility-based billing
.image images/UNT_green.png 3 850

IoT/Cloud computing convergence:

- Large, geographically distributed systems
- Edge devices exploit remote data centers and nearby _Cloudlets_
- Utility-based, on-demand resource consumption

* IoT Cloud Provisioning

Current IoT Cloud provisioning has focused on virtualization solutions for Edge devices.
As these solutions support a specific task, they are not generic enough for abstraction.

*Rethink*:
1. Representing the IoT Cloud infrastructure resources.
2. Managing their configuration and deployment models.
3. Composing low-level resource components into usable infrastructures, capable to support novel application requirements.

* IoT Cloud Provisioning Middleware

*Their*Solution*:

- A generic, light-weight resource abstraction mechanism, based on software-defined gateways, which enables application-specific customization of Edge devices.
- Support for automated provisioning of Edge resources and application components in a logically centralized manner, via dynamically managed APIs.
- Flexible provisioning models that enable self-service, on-demand consumption of the Edge resources.

* Motivation & Background

* Related Work

_Cyber-foraging_ systems:

- Extensive work has been done to enhance *resource-constrained* (Edge/mobile) devices.
- Usually focused on *specific* tasks (computation/data offloading).
- Emphasis on algorithms, energy-efficiency, and performance.

``
_Virtualizing/Abstraction_ of Edge devices:

- Sensors/actuators
- Management of device identification
- Device services aggregation
- Asynchronous event exchange (pub/sub model)

* SAPP: A Building Management System composed of Edge devices

.image images/middleware_sapp.png
.caption Solid arrows are sensor data; dashed boxes are IoT Cloud resources; dashed arrows show provisioning.

- Highly distributed, and executes atop both Edge devices and Cloud infrastructure.
- The BMS provider does not own physical infrastructure, so the Cloud resources are consumed as a utility & must be dynamically provisioned.

* Background

Previous work introduced a conceptual model for software-defined IoT Cloud systems.

The core concept of the provisioning model is the _software-defined_IoT_unit_.

_Software-defined_IoT_units_:

- Describe IoT Cloud resources (e.g., virtual sensors), their runtime environments (e.g., IoT gateways) and capabilities (e.g., communication protocols or data point controllers).
- Expose well-defined APIs
- Can be composed at different levels, creating virtual runtime infrastructure

Enables utility-based provisioning of IoT Cloud resources by providing a uniform view on the entire resource pool, as well as by allowing IoT Cloud applications to customize and consume those resources dynamically and on-demand.

* IoT Cloud Provisioning Middleware

* Provisioning Architecture

.image images/middleware_architecture.png 500 _
.caption Overview of the provisioning middleware

* Software-defined Gateways

- Artifact Packages
- Configurations Container
- Provisioning Agent
- Device Connectivity

.image images/middleware_sdg.png 320 _

* Edge Device Middleware Support

- Virtual Buffers Daemon
- Provisioning Daemon
- Monitoring Agent

* Cloud-based Provisioning Controller

- API Manager
- Monitoring Coordinator
- SDG Manager / Artifacts Manager
- Deployment Handler
- Dependency Management service

* Runtime Mechanisms for Multi-level Provisioning in IoT Cloud

* Runtime execution of provisioning workflows

.image images/middleware_sapp_workflow.png _ 440
.caption Provisioning workflow for SAPP

* Implementation & Evaluation

* Implementation

- Provisioning Controller written in Java and Scala
- Agents/_ProvisioningDaemon_ in Shell and Python scripts
- _VirtualBuffersDaemon_ implemented in Java SE Embedded

.image images/middleware_gateway.png 350 _
.caption An example of the gateway for a Building Management System.

* Performance

- CPU consumption averages 2%
- Java JVM processes consume 45Mb RAM
- Enables scalable execution of provisioning workflows (within _O(nlog(n))_)

* CPU consumption

.image images/middleware_perf1.png 500 _
.caption CPU consumption of the VirtualBuffersDeamon.

* Provisioning scalability

.image images/middleware_perf2.png 500 _
.caption Average execution time of provisioning workflows for JAPP and SAPP aplications.

* Conclusion

* Conclusion

The authors introduced a provisioning middleware that enables developing generic, multi-level provisioning workflows and supports automated and scalable execution of such workflows in IoT Cloud systems.

Their middleware supports on-demand, self-service resource consumption by providing flexible provisioning models and support for uniform, logically centralized provisioning of Edge devices, application artifacts, and their configuration models.

They introduced provisioning support for software-defined gateways to enable application-specific customization of Edge devices through well-defined APIs, while preserving the benefits of proven virtualization mechanisms.

* 2. ECC: Edge Cloud Composites

* Organization

1. Introduction

2. Motivating Uses Cases

3. ECC Concepts

4. Design

5. Implementation Details

6. Evaluation

7. Conclusions

* Introduction

* Current mobile computing

To obtain long battery lifetimes, mobile devices are designed with:

- Energy-efficient processors for low energy consumption
- Limited memory capacities for cost/space requirements
- On-device active power management for longer battery life
Intentionally *sacrifice* the _potential_ levels of computational ability and functionality _attainable_ within their current form factors.

*Cloud-based* _services_ run by companies like Google, Apple, Amazon, etc., offer one means for applications to *escape* the limits imposed by individual devices, yet at the same time, users are surrounded by *numerous* other _networked_devices_.

Surprisingly, most of those devices do *not* interoperate _seamlessly_, with each other, thereby *compromising* theoretically achievable computational efficiency and more importantly, end user’s *potential*experiences*.

* Reasons for fragmentation

- Hardware-software incompatibilities.
- No standardized protocols.
- Knowledge about the context in which a device is operating.
: which nearby devices are currently accessible and usable
- Information about nearby device capabilities.
- Instructions about the task at hand.
- Data about the steps needed to combine and jointly use the capabilities of multiple devices.

* ECC Software Architecture

.image images/ecc_architecture.png 540 _

* ECC Abstractions

The authors' research explores the utility and efficient use of '_nearby_devices_' for *enhancing* the capabilities of end users’ mobile devices and experiences.

They borrow from data center systems the notions of *resource*consolidation*, *elasticity*, and *dynamic*orchestration* of many machines’ computational capabilities, with minimum manual intervention.

The implicit knowledge about tasks to be carried out, resource accessibility, and capabilities plays a vital role in Cloud computing to achieve benefits of these notions.

.image images/UNT_green.png 3 850

*C*(ompetence) captures the functional capabilities of accessible devices and/or remote services.
*I*(ntent) articulates end user desires.
*C*(ontext) describing the current operating environment.

* Motivating Uses Cases

* Scenario

.image images/ecc_scenario.png
.caption A user wants to play a video in their home-environment.

* ECC benefits

- Seamless use of accessible devices.
- End-user device consolidation.
- Leveraging locality in tasks.

* ECC Concepts
: Intuitively, ECC articulates (i) what a user wants to do, (ii) which resources are available, and (iii) what those resources’ abilities are.

* Device Competences

*Competence* is defined as a tuple representing a device’s exposed _functionality_, _characteristics_, _availability_ and _accessibility_.
.image images/UNT_green.png 3 850

*Divided*into*two*classes*:

_Static_

- Functionality
- Characteristics

_Dynamic_

- Availability
- Accessibility

* User Intent

*Intent* is defined as an ordered sequence of events on _'partially_specified'_ competences linked by guidelines.
.image images/UNT_green.png 3 850

*ECC*Intent*:

- The set of tasks (stated as required competences)
- Their interactions, required to attain some higher level user goal (guideline)

Guidelines specify:

- Topology: the connection configuration between competence instances
: e.g., linear, many to one, etc.

- Traversal: referring to how events travel among competences
: e.g., synchronous, asynchronous, listening, etc.

- Tie: constraining certain competences to be instantiated on a specific device
: the affinity or anti-affinity of a competence to a device

* Context

*Context* is defined as the set of currently accessible competences.

.image images/UNT_green.png 3 850

Context captures current device availability and states, including their status due to multi-tenant operation, mobility constraints, current location, etc.

ECC is distributed which means that Context is a distributed runtime entity and is affected by current availability and accessibility.

*Competences*ECC*distinguishes*:

- Offered (same device)
- Local (nearby device on same network)
- Remote (device or a remote cloud accessible via the Internet)
- Cached (recently provisioned in previously requested intent)

* Design

* Operation

- ECC Application
- ECC Competence Instance
- ECC Decision Engine

*Reconfiguration*and*Failure*mechanisms*:
In case of context change, or if any instance is not meeting the quality constraints as specified in intent, reconfiguration mechanisms are triggered.

* Implementation Details

- ECC Implementation (C)
- Android-specific Details
: JAVA applications, JNI based interface for native Android APIs
: Had to root the device to get around unresponsive applications being killed by the OS
- Required Competences for Usecases
: OpenCV for video, X11-tst/Android native input API for collaborative UI

* Evaluation

* Distributed video playback scenario in three different contexts

Competences are offered by:

- Only a single device (i.e., the tablet)
- Multiple devices (i.e., tablet and nearby laptop)
- One nearby and one remote device (i.e., the tablet and an Amazon EC2 High CPU Medium instance)

* Role of 'Intent' (performance based decisions are not sufficient)

.image images/ecc_perf1.png 500 _
.caption Performance benefits with linear synchronous intent guideline.

* Reactivity (runtime changes in context)

.image images/ecc_perf2.png 500 _
.caption Time spike due to ECC reconfiguration from change in context.

* Low ECC overhead

.image images/ecc_perf3.png 500 _
.caption Total execution time - ECC's queues and actual computation.

* Same power, but lower energy consumption

.image images/ecc_energy.png 500 _
.caption No real difference in power utilization, but ECC reduces total energy due to lower execution time.

* Conclusions

* Conclusions

*ECC* offers *C*(ompetence) *I*(ntent) *C*(ontext) as first class concepts, making it possible for end users to enhance the capabilities of their own devices with additional nearby devices and remote cloud resources.

*ECC*-composed applications are shown to provide better performances and can also hide from users the unpredictable internet latencies inherent in remote cloud access.

_Limitations_:

- ECC Applications must be preprogrammed once with a set of competences
- No automatic user intent capture (nor formal specification of intent/competences)
- No learning capabilities in the decision engine to improve based on user behavior

* 3. ImageNet Classification with Deep Convolutional Neural Networks

* Organization

1. Introduction

2. The Dataset

3. The Architecture

4. Reduce Overfitting

5. Details of learning

6. Results

7. Discussion

* Introduction

Better object recognition with machine learning (ML):

- Learn on larger datasets
: wider range of examples
- Utilize more powerful models
: For faster training of large capacity models
- Better techniques to guard against overfitting
: To reduce error rates on test sets/unseen images

* Introduction (continued)

Even with a huge dataset, real images are far too invariable to be classified accurately.

The model should compensate for data it doesn't have with prior knowledge.

Convolutional Neural Networks (CNN) are one such class of model:

- Capacity can be controlled by changing their depth and breadth. (Fewer parameters than standard feed-forward neural networks and are easier to train.)

- GPU hardware is now powerful enough to train CNNs in a comfortable amount of time.

- Recent datasets like ImageNet contain enough labeled examples to train without overfitting.

* The Dataset

* ImageNet & ImageNet Large-Scale Visual Recognition Challenge

*ImageNet*:

- >15 million labeled high-resolution images
- 22,000 categories
- Labeled by humans using Amazon's Mechanical Turk

*ILSVRC*:

- Started in 2010
- Uses ~1,000 images in 1,000 categories of ImageNet
- 1.2 million training images
- 50,000 validation images with 150,000 testing images
- Two error rates reported: _top-1_ and _top-5_

.caption _top-5_ is the % of test images for which the correct label is not in the 5 labels considered most probable.

* The Architecture

* What is CNN?

- Inspired by animal visual systems (individual cortical neuron response)
- Based on multilayer perceptron models (feed-forward artificial neural network)
- Composed of one or more convolutional layers, along with pooling layers
- Convolutional layers are _normally_ not fully connected (only connected to local field)
- Shared weights so any layer can detect the same feature anywhere

.image images/cnn_diagram.png 255 _

* Novel features introduced

- Rectified Linear Units (ReLUs) for neuron models (faster training)
- Training on multiple GPUs in 1 system
- Local Response Normalization
- Overlapping Pooling

* Diagram

.image images/cnn_architecture3.png 500 _

: Eight learned layers, five convolutional and three fully-connected.
: Network training is split across two GPUs.
: Half of the kernels (or neurons) on each GPU, with one additional trick: the GPUs communicate only in certain layers.
: The kernels of layer 3 take input from all kernel maps in layer 2.
: Kernels in layer 4 take input only from those kernel maps in layer 3 which reside on the same GPU.

* Reduce Overfitting

* Data Augmentation with label-preserving transformations (1)

*Training/Validation*:
1. Take one of the original 256×256 px input images.
2. Extract random 224×224 px patches from it and horizontal reflections.
3. Train on these extracted (labeled) patches.

Increases training set by a factor of 2048, although highly inter-dependent.

*Testing*:
1. Extract five 224x224 patches (each corner and a center).
2. Add each reflection (for ten total patches).
3. Average the prediction made by the 1000-way _softmax_ layer on those ten patches.

* Data Augmentation with label-preserving transformations (2)

- Alter the intensities in the RGB channels using Principle Component Analysis.
- ~Captures an important property of natural images...object identity is invariant to changes in intensity and color of illumination.

Reduces _top-1_ error rate by over 1%.

All transformations are created off GPU on the CPU, so they are _"computationally_free"_.

* Data Augmentation with dropout

*Training/Validation*:
1. Set to 0 the output of each hidden neuron with a 50% probability.
2. "Dropped out" neurons do not contribute to the forward pass, or back-propagation.
3. Reduces complex co-adaptations of neurons, forced to learn more robust features.

*Testing*:
1. Use all the neurons, but multiply their output by 0.5 (approximate mean dropout).

Dropout is used in first two fully-connected layers and roughly doubles the number of iterations required to converge.

* Details of learning

- Stochastic gradient descent (batch: 128; momentum: 0.9; weight decay: 0.0005)
: Small weight decay was important for the model to learn, reducing the model's error rate
- Weights initialized with 0-mean Gaussian distribution with standard deviation 0.01
- 2nd, 4th, 5th layers initialized with constant 1 to accelerate ReLUs with positive input.
- Equal learning rate for all layers (adjusted manually throughout training)
- Trained for 90 cycles through the training set of 1.2 million images

Training took five to six days on two NVIDIA GTX 580 3GB GPUs.

* Results

* ILSVRC 2010 / 2012

.image images/cnn_results1.png
.caption ILSVRC-2010 test set. In _italics_ are best results achieved by others.

.image images/cnn_results2.png
.caption Error rates on ILSVRC-2012 validation and test sets. In _italics_ are best results achieved by others.
.caption Models with asterisk* were "pre-trained" to classify the entire ImageNet 2011 Fall release.

* Qualitative Evaluations (1)

.image images/cnn_kernels.png
.caption 96 convolutional kernels of size 11x11x3. The top 48 were learned on GPU1, while the bottom 48 were GPU 2.

The kernels on GPU 1 are largely color-agnostic, while the kernels on on GPU 2 are largely color-specific.
``
This kind of specialization occurs during every run and is independent of any particular random weight initialization (modulo renumbering of the GPUs).

* Qualitative Evaluations (2)

.image images/cnn_top_five.png 500 _
.caption Eight ILSVRC-2010 test images and the five labels considered most probable by their model.
: The correct label is written under each image, and the probability assigned to the correct label is also shown with a red bar (if it happens to be in the top 5).
: Even the mite that is off-center has been correctly labeled.
: Notice that there is some ambiguity about the intended focus (grille, cherry)

* Qualitative Evaluations (3)

.image images/cnn_closest_images.png 500 _
.caption Five ILSVRC-2010 test images in the first column and the six closest feature vector matches.
: The remaining columns show the six training images that produce feature vectors in the last hidden layer with the smallest Euclidean distance from the feature vector for the test image.

* Discussion

A large, deep convolutional neural network is capable of achieving record-breaking results on a challenging dataset.

Depth is important, removing any middle layer reduces performance *~2%* for _top-1_.

Still, they cannot match the human visual system until *larger/faster* networks are developed.


* In summary

_A_Middleware_Infrastructure_for_Utility-based_Provisioning_of_IoT_Cloud_Systems_

- An IoT middleware capable of automated, scalable deployments to the Cloud.
- Most IoT Cloud papers focus on Cyber-foraging or virtualization/abstraction.

_ECC:_Edge_Cloud_Composites_

- Defines virtual computational platforms composed of competences, context, and intent.
- Interesting concept for abstracting Edge devices to solve service fragmentation.

_ImageNet_Classification_with_Deep_Convolutional_Neural_Networks_

- Novel features were added to a CNN, achieving great results on image classification.
- Very accessible paper detailing what goes into developing a "good" CNN.

* Moving towards dissertation topic

Augmented reality (AR) with _Deep_Learning_:

- CNN classification engine
- Processing pipeline through *Cloud* or *Edge* devices
- Efficient switching for computational needs

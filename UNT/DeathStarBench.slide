DeathStarBench 
An Open-Source Benchmark Suite for Microservices and Their Hardware-Software Implications for Cloud & Edge Systems   (Gan et al, ASPLOS'19)
22 Nov 2019

Jacob Hochstetler
Ph.D. student, University of North Texas
jacobhochstetler@my.unt.edu
http://github.com/jh125486

* Organization

1. Introduction

2. Background & Related Works

3. DeathStarBench

4. Evaluation

5. Results

6. Conclusion

* Introduction: Monolith vs Microservices

.image images/microservices.png _ 620
.caption Differences in the deployment of monoliths and microservices

* Introduction: Microservices benefits

*Composable*software*design* 

- Simplifying and accelerating development
- System complexity can be managed at a modular level
- Facilitate deploying, scaling, and updating individual microservices

*Heterogenous*software*environment*

- Multiple programming languages/frameworks
- Each tier built with the most suitable language
- Services communication across well-defined planes (typically RPC or RESTful APIs)

*Simplify*correctness*and*performance*debugging*

- Bugs can be isolated in specific tiers
- Individual microservices can be delivered with SLA-based QoS

* Introduction: Measuring performance

Latency comes in two forms: _average_ latency and *tail* latency

.image images/deathstarbench_tail_latency.png 300 _

.link https://www2.cs.duke.edu/courses/cps296.4/fall13/838-CloudPapers/dean_longtail.pdf _The_Tail_at_Scale_ [Dean & Barroso, Communications of the ACM 2013]

*Queuing*theory*: Optimizing average latency maximizes throughput
But not the tail...
Shortening the tail reduces _queuing_latency_

* Background & Related Works

- _Cloudsuite_ 
: 1. includes both batch and interactive services, such as memcached, and has been used to study the architectural implications of cloud benchmarks

- _TailBench_
: 2. aggregates a set of interactive benchmarks, from web servers and databases to speech recognition and machine translation systems and proposes a new methodology to analyze their performance

- _Sirius_
: 3. also focuses on intelligent personal assistant workloads, such as voice to text translation, and has been used to study the acceleration potential for interactive ML applications

Single-tier applications, or at most services with two or three tiers, drastically deviate from the way cloud and edge services are deployed today. 

: For example, even applications like websearch, which is a classic multi-tier workload, are configured as independent leaf nodes, which does not capture correlations across tiers.

* DeathStarBench

Six end-to-end services (cloud and edge-based):

- Social network
- Media service (movie reviewing, renting, streaming)
- E-commerce site
- Secure banking system
- Swarm Cloud (an IoT service for control of drones)
- Swarm Edge 

* DeathStarBench (design principles)

- Representativeness
- End-to-end operation
- Heterogeneity
- Modularity
- Reconfigurability

* DeathStarBench (details)

Each benchmark service includes:

- tens of microservices in different languages and programming models
: 1. including node.js, Python, C/C++, Java, Javascript, Scala, and Go

- opensource applications
: 2. such as NGINX, memcached, MongoDB, Cylo, and Xapian

- custom RPC and RESTful APIs 
: 3. using popular open-source frameworks like Apache Thrift, and gRPC.

- lightweight and transparent distributed tracing system
: 4. similar to Dapper and Zipkin that tracks requests at RPC granularity

The study includes both traffic generated by real users of the services, and synthetic loads generated by open-loop workload generators.

* DeathStarBench (composition)

.image images/deathstarbench_composition.png _ 1020
.caption Characteristics and code composition of each end-to-end microservices-based application.

* DeathStarBench (edge drone swarm architecture)

.image images/deathstarbench_edge_swarm.png 520 _
.caption The Swam service running on edge devices and a local drone swarm executing the service

: the majority of the computation happens on the drones, including the motion planning, image recognition, and obstacle avoidance, with the cloud only constructing the initial route per-drone (Java service ConstructRoute), and holding persistent copies of sensor data. This architecture avoids the high network latency between cloud and edge, however, it is limited by the on-board resources. 
: The Controller and MotionController are implemented in Javascript, while ImageRecognition is using jimp, a node.js library for image recognition, and ObstacleAvoidance in C++. Services on the drones run natively, and communicate with each other over IPC, while the cloud and drones communicate over http to avoid installing the heavy dependencies of Thrift on the edge devices.
: swarm consists of 24 programmable Parrot AR2.0 drones together with a backend cluster of 20 two-socket, 40-core servers. Drones communicate with each other and the cluster over a wireless router

* Evaluation: Goals

1. How effective are current datacenter architectures are at running microservices

: 1. big vs small cores / determining the pressure microservices put on instruction caches / and exploring the potential they have for hardware acceleration

2. Networking and operating system implications of microservices

: 2. Specifically, similarly to traditional cloud applications, microservices spend a large fraction of time in the kernel. Unlike monolithic services though, microservices spend much more time sending and processing network requests over RPCs or other REST APIs

3. Complications of cluster management (or the mismanagement)

: 3. Even though the cluster manager can scale out individual microservices on-demand instead of the entire monolith, dependencies between microservices introduce backpressure effects and cascading QoS violations that quickly propagate through the system, making performance unpredictable. Existing cluster managers that optimize for performance and/or utilization are not expressive enough to account for the impact each pair-wise dependency has on end-to-end performance

4. Trade-offs in terms of application design and programming frameworks

: 4. quantify the performance trade-offs between RPC and RESTful APIs, and explore the performance and cost implications of running microservices on serverless programming frameworks

5. Tail at scale affects and explore the pressure they put on performance predictability

* Evaluation: Networking highlight

.image images/network_util.png _ 600 
.caption Network utilization comparison of three monolithic applications to one social network microservice [network in red]

* Evaluation: Focus on tail latency

"A system where each server typically responds in *10ms* but with a _99th_ percentile latency of *1s*...for a single-server architecture, only 1% of responses will be slow (at *1s*). 
If a request requires responses from 100 similar servers in parallel, then 63% of requests will take more than *1s*"

: Even for services with only one in 10,000 requests experiencing more than one-second latencies at the single-server level, a service with 2,000 such servers will see almost 20% requests taking more than one second.

.image images/deathstarbench_real_microservices.png 360 _
.caption Microservices graphs for three real production cloud providers and one from DeathStarBench (Social Network)

* Results

1. Architectural Implications

- Cycles breakdown and IPC
- I-cache pressure
- Brawny vs. wimpy cores

2. OS & Networking Implications

- OS vs. user-level breakdown
- Computation:communication ratio

3. Cluster Management Implications

* Results (continued)

4. Application & Programming Framework Implications

- Latency breakdown per microservice
- Serverless frameworks

5. Tail At Scale Implications

- Large-scale cascading hotspots
- Request skew
- Impact of slow servers

* Results: RAPL excerpt

Smaller microservices demonstrated much better instruction-cache locality than their monolithic counterparts. Logically these would be targeted for running on lower power.

.image images/deathstarbench_tail_latencies.png 440 _
.caption Tail latency with increasing load and decreasing frequency (RAPL). Lighter colors (yellow) denote QoS violations.
: RAPL is running average power limit, or a way to reduce CPU frequency (and therefore power consumption) on Intel CPUs

* Results: IoT excerpt

Edge vs cloud drone swarm latency comparison [image recognition/obstacle avoidance]

.image images/deathstarbench_swarm_latencies.png 400 _
.caption Throughput-tail latency for the Swarm service when execution happens at the edge versus the cloud

: Since drones have to communicate with a wireless router over a distance of several tens of meters,latencies are significantly higher than for the cloud-only services. 
: When processing happens in the cloud, latency at low load is higher, penalized by the long network delay.
: As load increases however, edge devices quickly become oversubscribed due to the limited on-board resources, with processing on the cloud achieving 7.8x higher throughput for the same tail latency, or 20x lower latency for the same throughput. 
: Obstacle avoidance shows a different trade-off, since it is less compute-intensive, and more latency-critical.
: Offloading obstacle avoidance to the cloud at low load can have catastrophic consequences if route adjustment is delayed, which highlights the importance of latency-aware resource management between cloud and edge, especially for safety-critical computation.

* Conclusion

Open-sourced and available at [[http://microservices.ece.cornell.edu]] (GPL licensed)
An Optimal Police Patrol Planning Strategy for Smart City Safety
14th IEEE International Conference on Smart City
14:30 12 Dec 2016
Tags: smartcity, security, entropy

Jacob Hochstetler, Lauren Hochstetler, Song Fu
University of North Texas
Texas, USA
jacobhochstetler@my.unt.edu
http://computerscience.engineering.unt.edu

* Overview

- Introduction

- Data sources

- Feature engineering / selection

- Resource allocation strategy

- Result

- Discussion

.image images/department_of_computer_science_and_engineering_green_sxs.png 100 _

: Funded in part by a National Science Foundation Grant
: Would like to thank my advisor Dr. Song Fu for his immense help, and my wife Lauren Hochstetler.
: She co-authored the paper as the domain in criminal justice.

* Introduction

City safety is an important aspect of any _Smart_City_.

Motorized and foot patrols have been shown to be an effective crime deterrent.

Smart, data-driven policing can reduce response times and _possibly_ reduce crime.

: An 2009 experiment in Philadelphia found that targeted foot patrols decreased violent crime 23% over a 12 week period.
: Possibly more importantly it raised the public's perception of police in the area, reducing their fear of crime.

* Problem

*Limited* number of police to patrol any given *area*.

: Treat police like any other city resource (water, electricity, etc)

* Goal

*Reduce* police response times while *increasing* patrol coverage.

: The primary goal is not to outright eliminate criminal activity, since those factors are wide-ranging and beyond this scope.

* Solution

Construct a _Police_Patrol_Network_ (PPN):

1. Cluster criminal activity into nodes.

2. Connect the nodes of the PPN.

3. Distribute patrols over the PPN.

* Data sources

: Two data sources are used to create the patrol networks:
: The nodes are created with the LA County crime dataset
: The edges between the nodes are the drive-time between nodes created from the Google Matrix API

* Data sources
Los Angeles County crime records

- Open dataset, available from [[http://sheriff.lacounty.gov]]
- From 2005 to present in CSV format
- "Wide" dataset with 18 attributes

2,130,504 crimes recorded from 2005 to 2015

.image images/la_sheriff.jpg _ 300

* Los Angeles County crime records

Chosen because of three main reasons:

- Weather stability
: to reduce weather-influenced crime factors

- Land size
: to allow for large drive time & distances

- Dataset size & crime variation
: over a decade of data, ranging from simple infractions to multiple homicides

.image images/Los_Angeles_County.png _ 300

* Data sources
`Google`Maps`Distance`Matrix`API`

- Provides driving time and distance between two points at any given time
- Uses current and historical traffic to predict travel duration
- Extensive client support (Java, Python, Go, NodeJS)
- Documentation at [[https://developers.google.com/maps/documentation/distance-matrix]]

.image images/google_maps_api.png _ 300

* Feature engineering / selection

: Many features are unnecessary to create our PPN.

* Feature engineering / selection

Selected features:

- INCIDENT_DATE (also includes the time prior to 2015)
- CATEGORY (42 named categories)
- X_COORDINATE (in feet)
- Y_COORDINATE (in feet)

Both the X and Y coordinates are in State Plane Coordinate System (SPS) format, specifically California Zone V.

Almost 30% of the original data had to be dropped due to incomplete/inconsistent/erroneous values.

: E.g. negative coordinates, coordinates outside LA county, subcategories that didn't match main categories, and zero-weight crimes

* Feature engineering / selection

Crime prioritization and analysis

- Obviously all crimes are not "equal" (vagrancy vs. homicide).

- Crime categories/subcategories can be leveraged to group crimes into levels.

- Some crimes don't require a reactive police response at all (e.g. serving warrants).

*Solution:*
Weight (0-5) each crime based on its category.

: A domain expert in Criminal justice was consulted to decide the number of weights and assign each category a weight.
: 0-weighted crimes don't require a patrol response and are removed from the dataset.
: Since many clustering algorithms don't take "weight" as a factor, we replicated each crime in the dataset by its respective weight.

* Crime distribution
.image images/by_day.png _ 600
.caption Distribution of crimes by day.

* Crime distribution
.image images/by_hour.png _ 600
.caption Distribution of crimes by hour.

* Crime distribution
.image images/by_weight.png _ 600
.caption Distribution of crimes based by weight/priority.

* Clustering example
: We use a simple data clustering method (K-Means), with a size of 50
: size 50 based on initial hexbin plotting.
: Better clustering options exist: Mclust or DBSCAN (no need to replicate weighted rows)

.image images/cluster0.png 540 _
.caption *50* c clusters generated by K-means for Monday at 0000 hours. *Black* dots represent each clusterâ€™s center (centroid).

* Clustering example

.image images/centroids_0000.png 540 _
.caption *50* crime clusters generated by K-means for Monday at 0000 hours. *Dot-size* based on aggregate crime weight.

* Connecting centroids (edges)

- The geometric distance between two centroids doesn't reflect real-world conditions.
: direct roads don't exist between every set of centroids

- Spatially-close centroids could be "far" apart.
: e.g. one-way roads

- Estimating the drive-time distance between two coordinates is inconsistent.
: e.g. speed limits, road routing

*Solution:*
Create cluster edges from the drive-time between centroids provided by the `Google`Matrix`API`.

_Additionally_, the `Google`Matrix`API` includes prediction traffic analysis and is time-aware.

* Resource allocation strategy

* Resource allocation strategy

For an area in question (such as a city or a county), given its historical crime data and available police resources, find the best officer placement for patrol that can maximally suppress possible crimes.

or...

An entropy maximization problem in a police placement network.

: Information theory use of entropy
Where entropy for a node is defined as:
.image images/entropy_node.png 75 _

: where wc1 is the aggregate weight of cluster 1
: wsys is the aggregate weight of the entire System
: rc1 is the quickest route from cluster 1 to any other cluster
: rsys is the quickest route in the entire system

* Resource allocation strategy
Entropy maximization

1. Calculate entire entropy of the system
2. Order the clusters by entropy contribution
3. Place an patrol at that cluster node
4. Adjust the entropy for the connected nodes of that cluster

Repeat until the entire area is covered or the number of available police is reached

* Result

* Result example

.image images/placed0.png 540 _
.caption *50* crime clusters for Monday at 0000 hours with *25* patrols placed. *Black*cross-hairs* show placed patrols.

: Note that in the south-east corner there are several outliers
: This means that the entropy formula must be balanced between the aggregate weighting and drive-time

* Result notes

Placed _N/2_ police patrols for a generalized solution

*Good:* Long drive-times are balanced with large crime clusters.

*Bad:* Empirically hard to implement for real-world testing

*Ugly*: Statistically hard to judge success based on crimes committed

* Discussion

* Discussion

Using available open data, we are able to create a smart data-driven approach to police patrol placement:

1. *Clustered* crimes based on their _location_ and weighted each cluster on the _severity_ of crimes there.

2. *Connected* clusters based on the _drive-time_ between each _pair_.

3. *Placed* police _patrols_ based on the _entropy_ gained.

Our entropy-based solution balances drive-time between crime clusters with the relative weight of the crimes at that cluster.

* Discussion

Future/additional work:

- Collapsing closely located clusters (within a certain patrol range).

- Tune number of clusters vs. number of police resources.

- Different cities could place emphasis on drive-time or cluster size.

- Weight older crimes less to better track current trends/hotspots

- Adjust patrol sizes to aggregate cluster weight and size.

- Expand coverage with the goal to reduce drive-time between clusters each hour.

